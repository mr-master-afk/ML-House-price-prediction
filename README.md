# ML-House-price-prediction
# სახლების ფასების პროგნოზირება - გაუმჯობესებული რეგრესიის ტექნიკები

ამ Kaggle კონკურსის მიზანია წინასწარ განვსაზღვროთ ამერიკელი ოჯახების გაყიდვების ფასი Ames, Iowa-ში არსებული საცხოვრებელი სახლების მონაცემების საფუძველზე. კონკურსი ითხოვს მონაწილეებისგან ხარისხიან feature engineering-ს, მოდელის შერჩევას და ჰიპერპარამეტრების ტიუნინგს, რათა მიღწეულ იქნას მაქსიმალურად ზუსტი პროგნოზირება.

---

## ჩემი მიდგომა

ამ პრობლემის გადასაჭრელად გამოვიყენე შემდეგი ნაბიჯები:

1. **მონაცემთა გაწმენდა**
    - წავშალე სვეტები, რომლებსაც 80%-ზე მეტი მონაცემი აკლდათ.
    - ამოვიღე გადამეტებულად კორელირებული სვეტები.

2. **ფიჩერების ინჟინერია**
    - შევქმენი ახალი სვეტები როგორიცაა `houseAge` და `houseremodelAge`.
    - ამოვიღე `YrSold`, `YearBuilt`, `YearRemodAdd` ფიჩერები, როგორც გამოუყენებელი.

3. **ფიჩერების შერჩევა**
    - გამოვიყენე კორელაციის ანალიზი რომ ამომეღო ზედმეტად კორელირებული სვეტები.

4. **წინასწარი დამუშავება**
    - რიცხვითი სვეტები: საშუალო იმპუტაცია + სტანდარტიზაცია.
    - კატეგორიული სვეტები: ყველაზე გავრცელებული მნიშვნელობით იმპუტაცია + one-hot კოდირება.
    - გავაერთიანე ColumnTransformer-ის გამოყენებით.

5. **მოდელირება**
    - გამოვცადე რამდენიმე მოდელი: Linear Regression, Ridge, Random Forest.
    - GridSearchCV-ით შევარჩიე ჰიპერპარამეტრები.
    - ავარჩიე საუკეთესო მოდელი RMSE-ის მიხედვით.

6. **ექსპერიმენტების ლოგირება**
    - გამოვიყენე MLflow და DagsHub რომ დამელოგა მოდელის პარამეტრები, მეტრიკები და ვიზუალიზაციები.
---
## ფაილები
   - model_experiment.ipynb - საბოლოო ფაილი, სადაც გავტესტე LinearRegression, RandomForestRegressor და Ridge
   - model_inference.ipynb - kaggl-ის ტესტ set-ის გატესტვა საუკეთესო მოდელზე
   - LinearRegression-more-cleaning-ft-selection.ipynb - ასევე წრფივი მოდელი შეცვლილი სვეტებით და მიდგომებით
   - LinearRegression-all-columns.ipynb - პირველი წრფივი მოდელი რომელიც გავტესტე ყველა სვეტზე one hot encoding-ით
   - დანარჩენი ფაილები ვიზუალიზაციისთვის და kaggle-ის დატასეტები
---

## შენიშვნები
   - ყველაზე კარგი შედეგი გამოყენებული მოდელებიდან აჩვენა LinearRegression-მა. Random Forest-ით მცირე დატაზე overfitting-ი მოხდა და შედეგებიც შესაბამისი იყო, ხოლო L2 იგივე Ridge რეგრესიის ცუდი შედეგი კი სავარაუდოდ გამოწვეული იყო იმ მიდგომით რომ რეალურად გამოსადეგ მაღალ კოეფიციენტებს ამცირებდა რაც bias ქმნიდა. cross-validation-მა იმისდა მიუხედავად რომ საუკეთესო პარამეტრები აარჩია საბოლოოდ პატარა დატასეტზე წრფივმა რეგრესიამ აჯობა დანარჩენებს.

